---
title: "hw8_mark"
author: "Ruizhou Peng"
date: "`r Sys.Date()`"
output: html_document
---

first load packages
```{r}
library(tidyverse)
```

## 1. Obstacles to valid scientific inference

**q1**:

1. Measurement distortions

It occurs when studying a variable which is hard to numerically measure. For example, when we explore the effect of drinking coffee to "concentration level", the concentration level is hard to measure by number. Instead of measure "concentration level" directly, we use proxy variables that are correlate to it. But the predicted proxy value may have difference to the true value. So it is measurement distortions.

2. Selection bias

It is a case when some samples are more likely to be selected than others so that the test by using samples to predict population may not be representative enough. There are four types of selection bias, those are "simple bias", "self select bias", "post hoc bias" and "attrition bias". 

3. Confounding variables

when exploring causal relationship between two variables, a third variable may effect both variables thus making these two variables correlate, however, we can not say that these two variables have causal relationship because of the troublemaker variable. 

## 2. Paired t-test and effect size

```{r eval=FALSE, include=FALSE}
install.packages("PairedData")
```

```{r}
library(PairedData)
data("Barley")
detach('package:PairedData', unload=TRUE)
detach("package:MASS", unload=TRUE)
```

```{r}
head(Barley, 4)
```

**q1**:

carry out a paired t-test to determine whether there is a difference in average yield between the two types of barley. Use a significance level of 0.01

```{r}
# first use t-test 
t.test(Barley$Glabron, Barley$Velvet, paired = TRUE, conf.level = 0.01)
```

**q2**:

compute the effect size using Cohen's d statistic

```{r}
# D = mean/sd

# generate a column of Difference
paired_barley <- Barley%>%
  mutate(diff=Glabron-Velvet)

# compute mean and sd
d_mean <- mean(paired_barley$diff, na.rm = TRUE)
d_sd <- sd(paired_barley$diff, na.rm=TRUE)

effect_size <- d_mean/d_sd

effect_size
```

as we can see, the effect size is bigger than 0.8, so it has strong effect size

**q3**:

the variable of difference should be fitted to Gaussian distribution

```{r}
# check if the diff fit Gaussian distribution

# use density plot
paired_barley%>%ggplot()+geom_density(aes(x=diff))

# use qqplot
paired_barley%>%ggplot(aes(sample=diff))+stat_qq()+stat_qq_line()

```

the density plot seems not look like a Gaussian distribution, but from qqplot, it fits Gaussian distribution well, the reason for the dislike by density plot is that the size of samples is small, there is only 12 samples.

Anyway, the difference variable satisfies the assumption.


## 3, Implementing unpaired t-test

create a function **t_test_function** which implements an unpaired Student's t-test.

first create a data frame **peng_AC** which is a subset of the Palmer penguins data set consisting those penguins which are **Adelie** or the **Chinstrap**

```{r}
library(palmerpenguins)
peng_AC <- penguins%>%
  # remove missing values
  drop_na(species, body_mass_g)%>%
  filter(species != "Gentoo")

head(peng_AC%>% select(species, flipper_length_mm, body_mass_g), 5)
```
before doing unpaired t test, we should validate the body_mass_g of two species obeys Gaussian distribution

```{r}
# use qq plot to show
ggplot(data=peng_AC%>%select(species,body_mass_g)%>%filter(species=="Adelie"), aes(sample=body_mass_g, legend='Adelie'))+stat_qq()+stat_qq_line()

ggplot(data=peng_AC%>%select(species,body_mass_g)%>%filter(species=="Chinstrap"), aes(sample=body_mass_g, legend='Chinstrap'))+stat_qq()+stat_qq_line()
```
Both Adelie and Chinstrap shows distributions likely to Gaussian distribution, so we can use t test.


**q1,2**:

first, understand what the following code is doing
```{r}
val_col <- "body_mass_g"
group_col <- "species"

data <- peng_AC

data_new <- data%>%
  # rename the columns
  # use "!!" to inject the constant variable into expression
  rename(group=(!!group_col), val=(!!val_col))%>%
  group_by(group)%>%
  drop_na(val)%>%
  summarise(mn=mean(val))

data_new
data_new$mn[2]
```

it first renames the **body mass g** and **species** columns, and then computes mean value grouped by species

now create the function **t_test_function**, there are several arguments:

1. "data":  A data frame argument

2. "val_col": A string argument, for column name for a continuous variable

3. "group_col": A string argument, for column name for a binary variable

4. "var_equal": A boolean argument, for whether assume the sds of X and Y are equal

the function should return a data frame contains the test statistic, p-value and effect size

$$
t_{equal} = \frac{\overline{X}-\overline{Y}}{S_{X,Y}\sqrt{\frac{1}{n_x}+\frac{1}{n_y}}},\quad
freedom_{equal}=n_x+n_y-2\\
t_{notequal} = \frac{\overline{X}-\overline{Y}}{\sqrt{\frac{S_X^2}{n_x}+\frac{S_Y^2}{n_y}}},\quad
freedom_{notequal}=\frac{(\frac{S_X^2}{n_x}+\frac{S_Y^2}{n_y})^2}{\frac{(S_X^2/n_x)^2}{n_x-1}+\frac{(S_Y^2/n_y)^2}{n_y-1}}\\
effsize=\frac{\overline{X}-\overline{Y}}{S_{X,Y}},where\;S_{X,Y}^2=\frac{(n_x-1)(S_X^2)+(n_y-1)(S_Y^2)}{n_x+n_y-2}
$$

```{r}
t_test_function <- function(data, val_col, group_col, var_equal){
  # data: data frame type
  # val_col: string
  # group_col: string
  # var_equal: boolean
  # return: data frame with t_statistic, p-value and effect size
  
  # compute mean, sd, size of each group
  compute_df<- data%>%
    rename(group=(!!group_col), value=(!!val_col))%>%
    drop_na(value)%>%
    group_by(group)%>%
    summarise(mn=mean(value),
              sd=sd(value),
              size=n())
  
  # show it
  # print(head(compute_df))
  
  # compute t_statistic, p-value and effect size
  
  if(!var_equal){
    # use Welch's t_test, var not equal
    t_stat <- (compute_df$mn[1]-compute_df$mn[2])/sqrt(compute_df$sd[1]^2/compute_df$size[1]+compute_df$sd[2]^2/compute_df$size[2])
  
    freedom <- (compute_df$sd[1]^2/compute_df$size[1]+compute_df$sd[2]^2/compute_df$size[2])^2/((compute_df$sd[1]^2/compute_df$size[1])^2/(compute_df$size[1]-1)+(compute_df$sd[2]^2/compute_df$size[2])^2/(compute_df$size[2]-1))
  
    # if t-stat > 0: P_val = 2*(1-pt)
    # else: P_val = 2*pt
    p_val <- case_when(t_stat>0~2*(1-pt(t_stat, df=freedom)),
                       t_stat<=0~2*(pt(t_stat, df=freedom))) 
  
    S_XY <- sqrt(((compute_df$size[1]-1)*compute_df$sd[1]^2+(compute_df$size[2]-1)*compute_df$sd[2]^2)/(compute_df$size[1]+compute_df$size[2]-2))
    
    # don't know what is the effect size for Welch's t test
    effect_size <- (compute_df$mn[1]-compute_df$mn[2])/S_XY
  }
  else{
    # use student t_test, var are equal
    
    S_XY <- sqrt(((compute_df$size[1]-1)*compute_df$sd[1]^2+(compute_df$size[2]-1)*compute_df$sd[2]^2)/(compute_df$size[1]+compute_df$size[2]-2))
    
    # print(S_XY)
    
    t_stat <- (compute_df$mn[1]-compute_df$mn[2])/(S_XY*sqrt(1/compute_df$size[1]+1/compute_df$size[2]))
    
    freedom <- compute_df$size[1]+compute_df$size[2]-2
    
    # print(freedom)
    
    p_val <- case_when(t_stat>0~2*(1-pt(t_stat, df=freedom)),
                       t_stat<=0~2*(pt(t_stat, df=freedom)))
    
    effect_size <- (compute_df$mn[1]-compute_df$mn[2])/S_XY
    
  }
  
  
  return(data.frame(t_stat, p_val, effect_size))
}

```

check, student's unpaired t test:

```{r}
t_test_function(data=peng_AC, val_col = "body_mass_g", group_col = "species", var_equal=TRUE)
```
Welch's unpaired t test:

```{r}
t_test_function(data=peng_AC, val_col = "body_mass_g", group_col = "species", var_equal=FALSE)
```

compare with t.test
```{r}
t.test(body_mass_g~species, data=peng_AC, var.equal=TRUE)
t.test(body_mass_g~species, data=peng_AC, var.equal=FALSE)
```

## 4. Useful concepts in statistical hypothesis testing

**q1**:

1. null hypothesis: It is a default situation we test to decide whether or not we can reject it

2. alternative hypothesis: when the test result shows we can reject null hypothesis, we turn to accept alternative hypothesis, normally we set alternative hypothesis represents some interesting phenomena

3. test statistic: this is the numerical measurement we use to act as extreme value

4. type1 error: it is the case when we reject null hypothesis when null hypothesis is right

5. type2 error: it is the case when we accept null hypothesis when alternative hypothesis is right

6. the size of a test: it is the same as the probability of type1 error, also called significance level

7. the power of a test: it is the probability of 1-type2, it is the ability to take as much as or more than the extreme value 

8. the significance level: the same as the size of a test

9. the p-value: it is the probability to take as large as extreme value or more than that

10. effect size: when rejecting null hypothesis and accept alternative hypothesis, effect size is use to measure how much degree can we trust the effect of one variable are on the other variable


**q2**:

(1) false, the p-value is the probability of wrongly reject null hypothesis, but we can't say the probability that the null hypothesis is true.

(2) false, we just can say we don't have good evidence to prove the null hypothesis is wrong


## 5. Investigating test size for an unpaired Student's t-test

**q1**:

```{r}
# various significance level
alphas <- seq(0.05, 1, 0.05)
num_trials <- 10000
sample_size <- 30
mu_0 <- 1
mu_1 <- 1
sigma_0 <- 3
sigma_1 <- 3
```

```{r}
set.seed(0)

# let compute one alpha be a function
compute_one_alpha_simulation <- function(alpha, num_trials, sample_size, mu_0, mu_1, sigma_0, sigma_1){
  # giving a alpha, compute the simulation type1 error
  
  single_alpha_test_size_simulation<- data.frame(trial=seq(num_trials))%>%
    # generate samples
    mutate(sample_0=map(trial, ~rnorm(sample_size, mu_0, sigma_0)),
           sample_1=map(trial, ~rnorm(sample_size, mu_1, sigma_1)))%>%
    # compute p value using t test
    mutate(p_values=pmap_dbl(list(sample_0, sample_1),
                         .f=~t.test(..1,..2,var.equal = TRUE, conf.level = 1-alpha)$p.value))%>%
    # compute type 1 error
    mutate(type_1_error=p_values<alpha)%>%
    summarise(mean_type_1_error=mean(type_1_error))
  
  return(single_alpha_test_size_simulation$mean_type_1_error)
}

# check
# compute_one_alpha_simulation(0.01, num_trials, sample_size, mu_0, mu_1, sigma_0, sigma_1)
```

now let's compute for difference alphas

```{r}
# compute multi alphas
alphas_test_size_simulation_df <- data.frame(alphas=alphas)%>%
  # use function to simulate for each alpha
  mutate(mean_type_1_error_for_each_alpha=map_dbl(alphas, ~compute_one_alpha_simulation(.x, num_trials, sample_size, mu_0, mu_1, sigma_0, sigma_1)))

# use line plot
alphas_test_size_simulation_df%>%ggplot(aes(x=alphas,y=mean_type_1_error_for_each_alpha))+geom_line()
```


it seems like a symmetric linear relationship between alphas and the average type 1 error probability, which is y=x. So the test size(also known as significance level) is the same as the probability of type 1 error.


## 6. The statistical power of an unpaired t-test

**q1**:

conduct a simulation study to explore how the statistical power varies

it is the same as test size, however, we assume the alternative hypothesis is true here, and let mu1 != mu2

```{r}
mu_0 <- 3
mu_1 <- 4

sigma_0 <- 2
sigma_1 <- 2
# compute multi alphas
alphas_power_simulation_df <- data.frame(alphas=alphas)%>%
  # use function to simulate for each alpha
  mutate(mean_reject_for_each_alpha=map_dbl(alphas, ~compute_one_alpha_simulation(.x, num_trials, sample_size, mu_0, mu_1, sigma_0, sigma_1)))

# use line plot
alphas_power_simulation_df%>%ggplot(aes(x=alphas,y=mean_reject_for_each_alpha))+geom_line()
```

when two samples have different means, then the average power acts like a log as significance level varies


**q2**:

conduct a simulation study to explore how the power varies as a function of the difference in means $\mu_1-\mu_0$

```{r}
num_trials <- 1000
sample_size <- 30
# maintain significance level and change difference in mu
alpha <- 0.05

mu_0 <- 0
sigma_0 <- 3
mu_1 <- seq(mu_0-3*sigma_0, mu_0+3*sigma_0, 0.1)
sigma_1 <- 3

diff <- mu_1-mu_0

set.seed(0)

# explore how difference between mu can affect power
power_diff_mu_df <- crossing(diff, trials=seq(num_trials))%>%
  # index
  # mutate(index=1:n())%>%
  # sample
  mutate(sample_0=map(diff, ~rnorm(sample_size, mu_0, sigma_0)),
         sample_1=map(diff, ~rnorm(sample_size, .x+mu_0, sigma_1)))
```


```{r}  
power_diff_mu_df%>%
  # compute p-values
  mutate(p_values=pmap_dbl(list(sample_0, sample_1),
                       ~t.test(..1,..2,var.equal = TRUE, conf.level = 1-alpha)$p.value))%>%
  # whether reject null hypothesis
  mutate(reject_null=p_values<alpha)%>%
  group_by(diff)%>%
  summarise(mean_reject_null=mean(reject_null))%>%
  ggplot(aes(x=diff,y=mean_reject_null))+geom_line(linewidth=1)
```

since we know that alternative hypothesis is true in this experiment, so the probability of reject null hypothesis is equal to the power, so the power quickly drop to 0 when difference between two samples are small.


**q3**:

conduct a simulation study to explore how the power varies as population standard deviation changes $\sigma=\sigma_0=\sigma_1$

```{r}
num_trials <- 1000
sample_size <- 30
# maintain significance level and change difference in sigma
alpha <- 0.05

mu_0 <- 1
sigmas <- seq(1,10,0.05)
mu_1 <- 2
sigma_0 <- sigmas
sigma_1 <- sigmas


set.seed(0)

# explore how difference between mu can affect power
power_sigma_df <- crossing(sigmas, trials=seq(num_trials))%>%
  # index
  # mutate(index=1:n())%>%
  # sample
  mutate(sample_0=map(sigmas, ~rnorm(sample_size, mu_0, .x)),
         sample_1=map(sigmas, ~rnorm(sample_size, mu_1, .x)))
```

```{r}
power_sigma_df%>%
  # compute p-value
  mutate(p_values=pmap_dbl(list(sample_0,sample_1),
                           ~t.test(..1,..2, var.equal = TRUE, conf.level = 1-alpha)$p.value))%>%
  # reject null hypothesis
  mutate(reject_null=p_values<alpha)%>%
  # group by sigmas
  group_by(sigmas)%>%
  summarise(mean_reject=mean(reject_null))%>%
  ggplot(aes(x=sigmas,y=mean_reject))+geom_line(linewidth=1)
```

From this plot, we can see that when mu is different between two samples, the power of test drop in exponential rate to 0 as sigmas differ.


**q4**:

conduct a simulation study to explore how the statistical power varies as a function of the sample size

```{r}
# change sample size
num_trials <- 1000
sample_size <- seq(3, 300)

alpha <- 0.05

mu_0 <- 1
mu_1 <- 2

sigma_0 <- 3
sigma_1 <- 3

power_size_df <- data.frame(sizes=sample_size)%>%
  # since compute_one_alpha_simulation returns a double value
  # so use map_dbl is better than map, since it should be unlist
  mutate(mean_reject=map_dbl(sizes, ~compute_one_alpha_simulation(alpha,num_trials,.x,mu_0,mu_1,sigma_0,sigma_1)))
```

```{r}
# plot using line plot
power_size_df%>%ggplot(aes(x=sizes,y=mean_reject))+geom_smooth()

```

from this plot, we can see that when sample sizes goes up, the power grows too.


## 7. Comparing the paired and unpaired t-tests on paired data

we have two i.i.d. samples X1,...,Xn and Y1,...,Yn

Yi = Xi+Zi, where Zi~$N(\mu_z,\sigma_z)$, which has $\mu_Y = \mu_X +\mu_Z \quad and \quad \sigma_Y^2 = \sigma_X^2+\sigma_Z^2 $

conduct a simulation study to explore the power of these two approaches. May consider a setting in which n=30, $\mu_x=10,\sigma_X=5$, $\mu_Z=1,\sigma_Z=1$, consider a range of different significance level

```{r}
compute_power <- function(alpha, num_trials, sample_size, mu_0, mu_1, sigma_0, sigma_1, paired){
  # giving a alpha, compute the simulation type1 error
  
  single_alpha_test_size_simulation<- data.frame(trial=seq(num_trials))%>%
    # generate samples
    mutate(sample_0=map(trial, ~rnorm(sample_size, mu_0, sigma_0)),
           sample_1=map(trial, ~rnorm(sample_size, mu_1, sigma_1)))%>%
    # compute p value using t test
    mutate(p_values=pmap_dbl(list(sample_0, sample_1),
                         .f=~t.test(..1,..2,var.equal = FALSE, conf.level = 1-alpha, paired = paired)$p.value))%>%
    # compute power
    mutate(reject_null=p_values<alpha)%>%
    summarise(mean_reject=mean(reject_null))
  
  return(single_alpha_test_size_simulation$mean_reject)
}

# a range of significance level
alphas <- seq(0.05, 1, 0.05)
sample_size <- 30

mu_x <- 10
sigma_x <- 5

mu_z <- 1
sigma_z <- 1

mu_y <- mu_x + mu_z
sigma_y <- sqrt(sigma_x^2+sigma_z^2)

num_trials <- 5000
paired_unpaired_compare_df <- data.frame(alphas)%>%
  # first compute paired power
  mutate(paired=map_dbl(alphas, ~compute_power(.x, num_trials, sample_size, mu_x, mu_y, sigma_x, sigma_y, TRUE)))%>%
  # then compute unpaired power
  mutate(unpaired=map_dbl(alphas, ~compute_power(.x, num_trials, sample_size, mu_x, mu_y, sigma_x, sigma_y, FALSE)))%>%
  # compute the difference between them
  mutate(diff=paired-unpaired)

paired_unpaired_compare_df%>%
  ggplot(aes(x=alphas, y=diff))+geom_line()+ylim(-0.5,0.5)
```

it seems that using unpaired and paired methods, the power of test appears alike, the difference between them is small.